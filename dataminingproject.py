# -*- coding: utf-8 -*-
"""DataMiningProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NB8_iIPr7hD1lDcOouiSlhImSSPM2DBb

Choose any values you want, run all the code, and check back when completed. At the very bottom will 3 savings plans.

Necessary files for running:
- personal_transactions.csv
- fbc_data_2022.csv

If you plan on using you own transaction file to test the model, it will need columns. Any other columns are optional.
- Date
- Amount

You are welcome to use the file provided to run the code.
"""

savings_goal = 1000 # @param {type:"number"}
yearly_income = 60000 # @param {type:"number"}
num_parents = 1 # @param {type:"number"}
num_child = 0 # @param {type:"number"}

transaction_file = "personal_transactions.csv" # @param {type:"string"}

"""# Installing all necessary requirements"""

#Installing all of the requirements
#for dataframe manipulation
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

#regular expressoin toolkit
import re

#NLP toolkits
!pip install -U nltk
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

#for plotting expense categories later
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
import matplotlib
import matplotlib.ticker as ticker # for formatting major units on x-y axis

#for downloading BERT
!pip install sentence_transformers
from sentence_transformers import SentenceTransformer

#for finding most similar text vectors
from sklearn.metrics.pairwise import cosine_similarity

"""# Helper functions"""

#cleans the text in the dataset to fit a specific form
def clean_text_BERT(text):
  # Convert words to lower case.
  text = text.lower()

  # Remove special characters and numbers. This also removes the dates
  # which are not important in classifying expenses
  text = re.sub(r'[^\w\s]|https?://\S+|www\.\S+|https?:/\S+|[^\x00-\x7F]+|\d+', '', str(text).strip())

  # Tokenise
  text_list = word_tokenize(text)
  result = ' '.join(text_list)
  return result

# condenses all of the columns except category into one string
def condense_to_str(dataset):
  # Get all column names except 'Category' and 'Description'
  non_category_columns = [col for col in dataset.columns if col != 'Category'
                          and col != 'Description' and col != 'Date' and col != 'Amount']

  # Combine all non-category columns into a single 'Description' column (one string)
  dataset['Description'] = dataset.apply(lambda row: ' '.join(row.values[:-1].astype(str)), axis=1)

  # Drop the original non-category columns and description
  dataset.drop(non_category_columns, axis=1, inplace=True)
  return dataset

  #should leave the dataset with one string and a matching category

# replaces categories in Category column with expected string values
def replace_categories(dataset):
  category_mapping = {
      'Alcohol & Bars': "Food",
      'Auto Insurance': "Transportation",
      'Coffee Shops': 'Food',
      'Electronics & Software': 'Misc',
      'Entertainment': 'Misc',
      'Fast Food': 'Food',
      'Gas & Fuel': 'Transportation',
      'Groceries': 'Food',
      'Haircut': 'Misc',
      'Home Improvement': 'Misc',
      'Internet': 'Other Necessities',
      'Mobile Phone': 'Other Necessities',
      'Mortgage & Rent': 'Housing',
      'Movies & DVDs': 'Misc',
      'Music': 'Misc',
      'Restaurants': 'Food',
      'Shopping': 'Misc',
      'Television': 'Misc',
      'Utilities': 'Other Necessities',
      'Childcare': 'Childcare',
      'Healthcare': 'Healthcare',
      'Taxes': 'Taxes',
      'Credit Card Payment': 'Misc',
      'Paycheck': 'Income',
  }
  result = dataset.copy()
  result['Category'] = result['Category'].map(category_mapping)
  return result

"""# Load data and preprocess

if you want to load the dataset, you need to add it yourself. File called 'personal_transaction.csv' saved in Drive, but you need to go into files of the Colab, and upload the file to use it in project.
"""

# loads dataset of transactions
df_transaction_description = pd.read_csv('personal_transactions.csv')

# Pre-processes the data to fit expected format
df_transaction_description = condense_to_str(df_transaction_description)
df_transaction_description = replace_categories(df_transaction_description)
df_transaction_description = df_transaction_description.dropna()
print(df_transaction_description.head())

"""# Split data and clean"""

# Split the data into training and testing sets
X = df_transaction_description['Description']
y = df_transaction_description['Category']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.head())
print(y_train.head())

# clean the data (make the training data look how we want it to)
#text_raw = df_transaction_description['Description']
#text_BERT = text_raw.apply(lambda x: clean_text_BERT(x))
text_BERT = X_train.apply(lambda x: clean_text_BERT(x))

"""# Train the model"""

# train the model using the training data as it encodes each string
# to numeric values
bert_input = text_BERT.tolist()
model = SentenceTransformer('paraphrase-mpnet-base-v2')
embeddings = model.encode(bert_input, show_progress_bar = True)
embedding_BERT = np.array(embeddings)

df_embedding_bert = pd.DataFrame(embeddings)
df_embedding_bert.head()

"""# Test the model"""

# now applying the same cleaning to the testing data
text_test_raw = X_test
text_test_BERT = text_test_raw.apply(lambda x: clean_text_BERT(x))

# Apply BERT embedding (see what values the model will give to the testing data)
bert_input_test = text_test_BERT.tolist()
model = SentenceTransformer('paraphrase-mpnet-base-v2')
embeddings_test = model.encode(bert_input_test, show_progress_bar = True)
embedding_BERT_test = np.array(embeddings_test)

df_embedding_bert_test = pd.DataFrame(embeddings_test)

"""# Compare how accurate prediction is"""

#embeddings = trained data, embeddings_test = test data

classifier = SVC()
classifier.fit(embeddings, y_train)

y_pred = classifier.predict(embeddings_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}') #Accuracy of 1? really?? that dont seem right...

#Well it works so far so wooooo
#print("Predicted vs. Expected values:")
#for true_val, pred_val in zip(y_test, y_pred):
    #print(f"True Value: {true_val}, Predicted Value: {pred_val}")

"""# Calculate the predictions on all of the transactions"""

# If we decide to let user add their own transaction, do it here
# loads dataset of transactions
transaction_description = pd.read_csv(transaction_file)

# Pre-processes the data to fit expected format
transaction_description = condense_to_str(transaction_description)
transaction_description = replace_categories(transaction_description)
transaction_description = transaction_description.dropna()
user_X = transaction_description['Description']

text_test_raw = user_X
text_test_BERT = text_test_raw.apply(lambda x: clean_text_BERT(x))

# Apply BERT embedding (see what values the model will give to the testing data)
bert_input_test = text_test_BERT.tolist()
model = SentenceTransformer('paraphrase-mpnet-base-v2')
embeddings_test = model.encode(bert_input_test, show_progress_bar = True)
embedding_BERT_test = np.array(embeddings_test)

df_embedding_bert_test = pd.DataFrame(embeddings_test)



classifier = SVC()
classifier.fit(embeddings, y_train)

all_pred = classifier.predict(embeddings_test)

"""# Calulcate monthly averages"""

totals = transaction_description['Amount']
dates = transaction_description['Date']

from collections import defaultdict
import datetime

# Initialize a dictionary to store the total amounts for each (category, month) pair
category_month_totals = defaultdict(float)

# Assuming your arrays have the same length
for category, date, amount in zip(all_pred, dates, totals):
    # Convert the date string to a datetime object
    date_obj = datetime.datetime.strptime(date, "%m/%d/%Y")

    # Extract the year and month from the date
    year_month = (date_obj.year, date_obj.month)

    # Create a tuple key using (category, year_month)
    key = (category, year_month)

    # Update the total amount for that category and month
    category_month_totals[key] += amount

# Now, category_month_totals contains the total amount spent in each category for each month
#for key, total in category_month_totals.items():
   # category, year_month = key
    #year, month = year_month
    #print(f"Category: {category}, Month: {year}/{month}, Total Amount Spent: {total}")

"""# Installing all necessary requirements (start of FBC)"""

#Installing all of the requirements
#for dataframe manipulation
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop

#regular expressoin toolkit
import re

#NLP toolkits
!pip install -U nltk
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize

#for plotting expense categories later
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
import matplotlib
import matplotlib.ticker as ticker # for formatting major units on x-y axis

#for downloading BERT
!pip install sentence_transformers
from sentence_transformers import SentenceTransformer

#for finding most similar text vectors
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

"""# Helper functions FBC"""

# condenses all of the columns except category into one string
def condense_to_str1(dataset):
  # Get all column names except 'Category' and 'Description'
  non_category_columns = [col for col in dataset.columns if col != 'Housing'
                          and col != 'Food' and col != 'Transportation' and col != 'Healthcare'
                           and col != 'Other Necessities' and col != 'Childcare'
                          and col != 'Taxes' and col != 'num_parents' and col != 'num_children' and col != 'median_family_income']

  # Combine all non-category columns into a single 'Description' column (one string)
  # dataset['Description'] = dataset.apply(lambda row: ' '.join(row.values[:-1].astype(str)), axis=1)

  # Drop the original non-category columns and description
  dataset.drop(non_category_columns, axis=1, inplace=True)
  return dataset

  #should leave the dataset with one string and a matching category

# Define a function to split the 'family' column
def split_family1(family_str):
    parts = family_str.split('p')
    num_parent = int(parts[0])
    num_children = int(parts[1].replace('c', ''))
    return num_parent, num_children

def clean_values1(df):
    chars_to_remove = ['$', ',', '%', '+']
    # Create a regular expression pattern that matches any of the characters to remove
    pattern = '[' + ''.join(chars_to_remove) + ']'

    # Apply the cleaning
    for col in df.columns:
        if df[col].dtype == 'object':  # Apply only to columns with object datatype (string typically)
            df[col] = df[col].str.replace(pattern, '', regex=True)
        elif df[col].dtype == 'int' or df[col].dtype == 'float':
            # If the column is numeric, ensure it's in string format first
            df[col] = df[col].astype(str)
            df[col] = df[col].str.replace(pattern, '', regex=True)
            # Convert cleaned strings back to numeric type
            df[col] = pd.to_numeric(df[col], errors='coerce')

    return df

"""# Loading and Cleaning data"""

# loads dataset of transactions
fbc_data = pd.read_csv('fbc_data_2022.csv', skiprows=1)

# Pre-processes the data to fit expected format

fbc_data[['num_parents', 'num_children']] = fbc_data.apply(lambda row: split_family1(row['Family']), axis=1, result_type='expand')
fbc_data = condense_to_str1(fbc_data)
fbc_data = clean_values1(fbc_data)
fbc_data = fbc_data.dropna()
print(fbc_data.head())

scaler = MinMaxScaler()
fbc_scaled = scaler.fit_transform(fbc_data)

X_train, X_test = train_test_split(fbc_scaled, test_size=0.2, random_state=42)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))
X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))

print('x_train shape:', X_train.shape)
print('x_test shape:', X_test.shape)

"""# Autoencoder"""

# dimensionality of input and latent encoded representations
input_dim = 10
ltnt_dim = 2

# --- define an autoencoder model here
input_vec = Input(shape=(input_dim,))

fl1 = Dense(128, activation='relu')(input_vec)
fl1 = BatchNormalization()(fl1)

fl2 = Dense(64, activation='relu')(fl1)
fl2 = BatchNormalization()(fl2)

encoder = Dense(ltnt_dim, activation='relu') (fl2)   # Activation changed to relu

# model that takes input and encodes it into the latent space
latent_ncdr = Model(input_vec, encoder)

fl3 = Dense(64, activation='relu')(encoder)
fl3 = BatchNormalization()(fl3)

fl4 = Dense(128, activation='relu')(fl3)
fl4 = BatchNormalization()(fl4)

decoder = Dense(input_dim, activation='sigmoid') (fl4)

# model that takes input, encodes it, and decodes it
f_autoencoder = Model(input_vec, decoder)

f_opt = RMSprop(learning_rate=0.001)

f_autoencoder.compile(loss='mean_absolute_error', optimizer=f_opt)

f_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=10,
                              min_delta=1e-4, mode='min')

f_stop_alg = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)

hist = f_autoencoder.fit(X_train, X_train, batch_size=100, epochs=1000,
                   verbose=0, callbacks=[f_stop_alg, f_reduce_lr], shuffle=True,
                   validation_data=(X_test, X_test))

f_autoencoder.save_weights("autoencoder.hdf5")

import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10,6))
plt.plot(hist.history['loss'], color='#785ef0')
plt.plot(hist.history['val_loss'], '--', color='#dc267f')
# plt.yscale('log')
plt.title('Model reconstruction loss')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(['Training Set', 'Validation Set'], loc='upper right')
plt.show()

"""# Fuzzy C-means"""

!pip install fuzzy-c-means
# Step 1: Install the fuzzy-c-means package if not already installed.
# You can install it using pip:
# !pip install fuzzy-c-means

from fcmeans import FCM

c = 10
# Step 2: Preprocess the dataset
# You've already loaded the dataset into `fbc_data`.
# Now, you will select the 6 dimensions you want to use for clustering.
# Assuming these are the columns 'Housing', 'Food', 'Transportation', 'Healthcare', 'Childcare', 'Taxes'

# Select the relevant features
features = ['Housing', 'Food', 'Transportation', 'Healthcare', 'Other Necessities', 'Childcare', 'Taxes', 'median_family_income', 'num_parents', 'num_children']
f_features = fbc_data[features]

# Convert all columns to numeric type (if they are not already)
f_features = f_features.apply(pd.to_numeric, errors='coerce')

# Handle missing values
imputer = SimpleImputer(strategy='mean')  # you can change the strategy if needed
# df_features = pd.DataFrame(imputer.fit_transform(df_features), columns=df_features.columns)

# Normalize the data
scaler = MinMaxScaler()
f_features_scaled = scaler.fit_transform(f_features)

# Step 3: Perform fuzzy c-means clustering
# Initialize the FCM with the number of clusters you want
fcm = FCM(n_clusters=c)  # Change the number of clusters according to your needs

# Fit the fuzzy c-means
encoded_data = latent_ncdr.predict(f_features_scaled)
fcm.fit(encoded_data)

# Get the cluster centers and labels
centers = fcm.centers
labels = fcm.u.argmax(axis=1)

# Step 4: Analyze the results
# You might want to add the cluster labels to your original dataframe to see which item belongs to which cluster
fbc_data['Cluster'] = labels

# You can also check the distribution of items in each cluster
cluster_distribution = fbc_data['Cluster'].value_counts()

print(fbc_data.head())
print("Cluster centers:", centers)
print("Cluster distribution:\n", cluster_distribution)

import matplotlib.pyplot as plt

# Assuming `encoded_data` is 2-dimensional for visualization. If not, you should reduce it to 2 dimensions first.
# Plot the clustered data
plt.figure(figsize=(8, 5))
scatter = plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=labels, cmap='viridis', alpha=0.5)

# Plot the centroids
plt.scatter(centers[:, 0], centers[:, 1], s=300, c='pink', marker='.')

# Add legend and labels (adjust the legend if necessary)
plt.legend(handles=scatter.legend_elements()[0], labels=list(range(c)))
plt.title('Fuzzy C-Means Clustering')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')

# Show the plot
plt.show()

"""# Monthly average calculation"""

category_totals = {}  # To store total amount spent for each category
category_counts = {}  # To store the number of months for each category

for key, total in category_month_totals.items():
    category, year_month = key
    year, month = year_month

    # Update total amount spent for the category
    if category in category_totals:
        category_totals[category] += total
        category_counts[category] += 1
    else:
        category_totals[category] = total
        category_counts[category] = 1

    #print(f"Category: {category}, Month: {year}/{month}, Total Amount Spent: {total}")

category_avg = {}
# Calculate and print the monthly average for each category
for category in category_totals:
    total_amount = category_totals[category]
    total_months = category_counts[category]
    average = total_amount / total_months
    category_avg[category] = average
    print(f"Category: {category}, Monthly Average: {average}")

#dave_data = [Housing  Food Transportation Healthcare Other Necessities  Childcare Taxes median_family_income  num_parents  num_children]

dave_data = [category_avg['Housing'], category_avg['Food'], category_avg['Transportation'], category_avg['Healthcare'],
             category_avg['Other Necessities'], category_avg['Childcare'], category_avg['Taxes'], yearly_income, num_parents, num_child]
print(dave_data)

"""# Fitting to FCM"""

data_matrix = [dave_data]

new_data_point_standardized = scaler.transform(data_matrix)

encoded_new_data_point = latent_ncdr.predict(new_data_point_standardized)

# Predict the cluster assignment using the trained k-means model
predicted_cluster = fcm.predict(encoded_new_data_point)

print("Predicted Cluster:", predicted_cluster)

# Plot the clusters
plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=labels, cmap='viridis', label='Training Data')

# Plot the new data point in red
plt.scatter(encoded_new_data_point[0, 0], encoded_new_data_point[0, 1], c='red', marker='x', label='New Data Point')

# Add labels and legend
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('K-Means Clustering with New Data Point')
plt.legend()

# Show the plot
plt.show()

"""# Analysis of Fitting to FCM"""

# Analyze each cluster's characteristics
for cluster_num in range(c):
    cluster_data = fbc_data[fbc_data['Cluster'] == cluster_num].drop('Cluster', axis=1)

    # Ensure all columns are numeric
    cluster_data = cluster_data.apply(pd.to_numeric, errors='coerce')

    print(f"Cluster {cluster_num} Characteristics:")
    print(cluster_data.mean())
    print("\n-------------------\n")

"""# Calculating Budget

Last: Calculate his budget based on his neighbors
"""

def calc_percentages(category_avg, income):
  perc = {}

  for category in category_avg:
    per = (category_avg[category]*12) / income * 100
    perc[category] = per
    # print(f"Category: {category}, Monthly Percentage: {per}")
  return perc

dave_perc = calc_percentages(category_avg, yearly_income)
predicted_cluster

cluster_data = fbc_data[fbc_data['Cluster'] == predicted_cluster[0]].drop('Cluster', axis=1)
cluster_data = cluster_data.apply(pd.to_numeric, errors='coerce')

# Calculate the mean values for each category inside the cluster
cluster_avg_values = cluster_data.mean()
cluster_avg_dict = cluster_avg_values.to_dict()

# Access the mean value for the income variable
income_mean = cluster_avg_values['median_family_income']

# Pass the mean values and the income mean to calc_percentages function
cluster_perc = calc_percentages(cluster_avg_dict, income_mean)

def find_key_differences(arr1, arr2):
    common_keys = set(arr1.keys()) & set(arr2.keys())
    key_differences = {}

    for key in common_keys:
        value_difference = arr1.get(key, 0) - arr2.get(key, 0)
        key_differences[key] = value_difference

    return key_differences

result = find_key_differences(dave_perc, cluster_perc)
print(result)

weights = {
    "Housing": 0.01,
    "Healthcare": 0.001,
    "Childcare": 0.03,
    "Transportation": 0.02,
    "Other Necessities": 0.05,
    "Food": 0.04,
    "Taxes": 0.001
}

def calculate_savings_needed(percent_diff, yearly_income, savings_goal, months):
    cpy_perc_dif = percent_diff.copy()
    monthly_income = yearly_income / 12
    savings_needed = {category: 0 for category in percent_diff}

    while sum(savings_needed.values()) * months < savings_goal:
      for category, diff_percentage in cpy_perc_dif.items():
        if diff_percentage > 0:
          additional_spending = monthly_income * (diff_percentage / 100)
          amount_saved = additional_spending * weights[category]
          savings_needed[category] += amount_saved
          cpy_perc_dif[category] = cpy_perc_dif[category] - (amount_saved/additional_spending)
        else:
          abs_diff_percentage = abs(diff_percentage)
          reduction_factor = 1 / (1 + (abs_diff_percentage / 100))  # Adjust the factor as needed
          additional_spending = monthly_income * (abs_diff_percentage / 100) * reduction_factor
          amount_saved = additional_spending * (weights.get(category, 1) * reduction_factor)
          savings_needed[category] += amount_saved
          cpy_perc_dif[category] -= amount_saved / additional_spending


    return savings_needed

def calculate_savings_needed1(percent_diff, yearly_income, savings_goal, months):
    cpy_perc_dif = percent_diff.copy()
    monthly_income = yearly_income / 12
    savings_needed = {category: 0 for category in percent_diff}

    while sum(savings_needed.values()) * months < savings_goal:
      for category, diff_percentage in cpy_perc_dif.items():
        if diff_percentage > 0:
          additional_spending = monthly_income * (diff_percentage / 100)
          amount_saved = additional_spending * weights[category]
          savings_needed[category] += amount_saved
          cpy_perc_dif[category] = cpy_perc_dif[category] - (amount_saved/additional_spending)


    return savings_needed

savings_needed_per_category = calculate_savings_needed(result, yearly_income, savings_goal, 6)
print(savings_needed_per_category)
savings_needed_per_category = calculate_savings_needed1(result, yearly_income, savings_goal, 6)
print(savings_needed_per_category)

def display_dictionary_as_dollars(dictionary):
    max_key_length = max(len(str(key)) for key in dictionary.keys())
    print(f"{'Category':<{max_key_length + 5}} | Amount to save per month")
    print("---------------------------------------------------")
    for key, value in dictionary.items():
        print(f"{key:<{max_key_length + 5}} | ${value:.2f}")

"""# Savings Plans"""

savings_needed_per_category = calculate_savings_needed(result, yearly_income, savings_goal, 6)
print("Savings plan for 6 months")
print("---------------------------------------------------")
display_dictionary_as_dollars(savings_needed_per_category)
print()
savings_needed_per_category = calculate_savings_needed(result, yearly_income, savings_goal, 12)
print("Savings plan for 12 months")
print("---------------------------------------------------")
display_dictionary_as_dollars(savings_needed_per_category)
print()
savings_needed_per_category = calculate_savings_needed(result, yearly_income, savings_goal, 24)
print("Savings plan for 24 months")
print("---------------------------------------------------")
display_dictionary_as_dollars(savings_needed_per_category)
print()